{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNANrWc1aDJ8wm014aEjHpt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nackjaylor/sydney-innovation-program/blob/main/sip_unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning (Auto-Encoders)\n",
        "Load necessary dependencies."
      ],
      "metadata": {
        "id": "6gzS60eq7Q1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSsCyEMP-sCf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the MNIST Dataset (handwritten digits) and set some information about our network. We set number of epochs, batch size, capacity, learning rate for the optimisation and importantly the dimension of our latent space."
      ],
      "metadata": {
        "id": "aLqosNiURIf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 75\n",
        "batch_size = 64\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "latent_dimension = 16"
      ],
      "metadata": {
        "id": "nsdHzifXSP2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we're working with images, we want to normalise the output to the range 0-1 (this is a big trip up!). In actuality, sometimes we don't want this: we quantise a model to make it run faster and more efficiently."
      ],
      "metadata": {
        "id": "-FskU22G7kCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Qjaw23CbcBoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the MNIST dataset look like?"
      ],
      "metadata": {
        "id": "mxzc85L07zIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "4HXnOnBCc6th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a convolutional autoencoder. Have a look through the model, and confirm that it is the same forwards and backwards."
      ],
      "metadata": {
        "id": "xKd6zA6C72Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
        "        \n",
        "        self.fc = nn.Linear(in_features=c*2*7*7, out_features=latent_dimension)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.fc = nn.Linear(in_features=latent_dimension, out_features=c*2*7*7)\n",
        "        \n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        \n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered\n",
        "        return x\n",
        "    \n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon\n",
        "    \n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "autoencoder = autoencoder.to(device)"
      ],
      "metadata": {
        "id": "a_FCtS8KStnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our main training loop. We loop over a range of epochs, passing data as we go and updating the weights. \n",
        "\n",
        "Look at where we generate the loss, and see that we use the training data as what we compare to. An autoencoder is a function as below:\n",
        "\n",
        "F:xâ†¦x"
      ],
      "metadata": {
        "id": "54TEuqdX7_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# set to training mode\n",
        "autoencoder.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch, _ in train_dataloader:\n",
        "        \n",
        "        image_batch = image_batch.to(device)\n",
        "        \n",
        "        # autoencoder reconstruction\n",
        "        image_batch_recon = autoencoder(image_batch)\n",
        "        \n",
        "        # reconstruction error\n",
        "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, epochs, train_loss_avg[-1]))"
      ],
      "metadata": {
        "id": "RVh23QJlU0Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(autoencoder.state_dict())"
      ],
      "metadata": {
        "id": "90nyfYb8YlB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show how our model did!"
      ],
      "metadata": {
        "id": "KVqHnM_O8XKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gP51NpSZVQbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us evaluate the autoencoder."
      ],
      "metadata": {
        "id": "7Z_zQNYq8ZFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.eval()\n",
        "\n",
        "test_loss_avg, num_batches = 0, 0\n",
        "for image_batch, _ in test_dataloader:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # autoencoder reconstruction\n",
        "        image_batch_recon = autoencoder(image_batch)\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
        "\n",
        "        test_loss_avg += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "test_loss_avg /= num_batches\n",
        "print('average reconstruction error: %f' % (test_loss_avg))"
      ],
      "metadata": {
        "id": "iTA_cf-nWvb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils\n",
        "import numpy as np\n",
        "autoencoder.eval()\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        images = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = iter(test_dataloader).next()\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the autoencoder\n",
        "print('Autoencoder reconstruction:')\n",
        "visualise_output(images, autoencoder)"
      ],
      "metadata": {
        "id": "lKEqfxPrW0C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see what moving through different latent vectors allows us to do!"
      ],
      "metadata": {
        "id": "LeUkGjG78ifU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.eval()\n",
        "\n",
        "def interpolation(lambda1, model, img1, img2):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        # latent vector of first image\n",
        "        img1 = img1.to(device)\n",
        "        latent_1 = model.encoder(img1)\n",
        "\n",
        "        # latent vector of second image\n",
        "        img2 = img2.to(device)\n",
        "        latent_2 = model.encoder(img2)\n",
        "\n",
        "        # interpolation of the two latent vectors\n",
        "        inter_latent = lambda1 * latent_1 + (1- lambda1) * latent_2\n",
        "\n",
        "        # reconstruct interpolated image\n",
        "        inter_image = model.decoder(inter_latent)\n",
        "        inter_image = inter_image.cpu()\n",
        "    \n",
        "    return inter_image\n",
        "    \n",
        "# sort part of test set by digit\n",
        "digits = [[] for _ in range(10)]\n",
        "for img_batch, label_batch in test_dataloader:\n",
        "    for i in range(img_batch.size(0)):\n",
        "        digits[label_batch[i]].append(img_batch[i:i+1])\n",
        "    if sum(len(d) for d in digits) >= 1000:\n",
        "        break;\n",
        "\n",
        "# interpolation lambdas\n",
        "lambda_range=np.linspace(0,1,10)\n",
        "\n",
        "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
        "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "\n",
        "for ind,l in enumerate(lambda_range):\n",
        "    inter_image=interpolation(float(l), autoencoder, digits[7][0], digits[1][0])\n",
        "   \n",
        "    inter_image = to_img(inter_image)\n",
        "    \n",
        "    image = inter_image.numpy()\n",
        "   \n",
        "    axs[ind].imshow(image[0,0,:,:], cmap='gray')\n",
        "    axs[ind].set_title('lambda_val='+str(round(l,1)))\n",
        "\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "jHLO4kLqYI8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Models\n",
        "\n",
        "We can use random vectors to generate unseen data!"
      ],
      "metadata": {
        "id": "BD5WWAve8csD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # approx. fit a multivariate Normal distribution (with diagonal cov.) to the latent vectors of a random part of the test set\n",
        "    images, labels = iter(test_dataloader).next()\n",
        "    images = images.to(device)\n",
        "    latent = autoencoder.encoder(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    mean = latent.mean(dim=0)\n",
        "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, latent_dimension)*std + mean\n",
        "\n",
        "    # reconstruct images from the latent vectors\n",
        "    latent = latent.to(device)\n",
        "    img_recon = autoencoder.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QbkD2QBEYhIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}